# ğŸ•·ï¸ Go Web Crawler

A high-performance, concurrent web crawler built in Go that extracts structured data from websites and exports it to CSV format for easy analysis.

## âœ¨ Features

- ğŸš€ **Concurrent Crawling** - Configurable concurrency levels for optimal performance
- ğŸ¯ **Domain-Focused** - Stays within the target domain boundaries
- ğŸ“Š **Rich Data Extraction** - Captures page titles, content, links, and images
- ğŸ“ˆ **CSV Export** - Exports structured data for analysis in spreadsheets
- ğŸ›‘ **Smart Limits** - Configurable maximum page limits to prevent runaway crawls
- ğŸ”’ **Thread-Safe** - Built with Go's concurrency best practices
- âš¡ **Fast & Efficient** - Leverages goroutines for parallel processing

## ğŸ› ï¸ Installation

### Prerequisites
- Go 1.19 or higher
- Internet connection for crawling

### Setup
1. Clone the repository:
```bash
git clone <your-repo-url>
cd WebCrawler
```

2. Install dependencies:
```bash
go mod init webcrawler
go get github.com/PuerkitoBio/goquery
```

3. Build the crawler:
```bash
go build -o crawler
```

## ğŸš€ Usage

### Basic Command
```bash
./crawler <URL> <maxConcurrency> <maxPages>
```

### Parameters
- **URL** - The website to crawl (must include `http://` or `https://`)
- **maxConcurrency** - Number of concurrent requests (1-10 recommended)
- **maxPages** - Maximum number of pages to crawl (prevents runaway crawls)

### Examples

#### ğŸ“ Small Website Crawl
```bash
./crawler "https://example.com" 2 10
```

#### ğŸ¢ Medium Blog Crawl
```bash
./crawler "https://blog.boot.dev/" 3 25
```

#### ğŸŒ Large Site Crawl
```bash
./crawler "https://wagslane.dev" 5 50
```

### Using `go run`
You can also run directly without building:
```bash
go run . "https://example.com" 3 10
```

## ğŸ“‹ Output

### Console Output
```
starting crawl of: https://blog.boot.dev/
maxConcurrency: 3
maxPages: 25
crawling: https://blog.boot.dev/
crawling: https://blog.boot.dev/golang/
crawling: https://blog.boot.dev/python/
...

Generating CSV report: report.csv
Crawl completed: 25 pages found (max: 25)
Report saved to report.csv
```

### CSV Export
The crawler generates a `report.csv` file with the following columns:

| Column | Description | Example |
|--------|-------------|---------|
| `page_url` | Full URL of the crawled page | `https://blog.boot.dev/golang/` |
| `h1` | Main heading of the page | `"Learn Go Programming"` |
| `first_paragraph` | First paragraph of content | `"Go is a powerful language..."` |
| `outgoing_link_urls` | All links found on the page | `https://go.dev;https://golang.org` |
| `image_urls` | All images found on the page | `logo.png;banner.jpg` |

**Note:** Multiple links and images are separated by semicolons (`;`)

## ï¿½ï¿½ Project Structure

```
WebCrawler/
â”œâ”€â”€ main.go                     # ğŸ  Main application entry point
â”œâ”€â”€ concurrent_crawler.go       # ğŸ•¸ï¸ Core crawling logic and concurrency
â”œâ”€â”€ csv_report.go              # ğŸ“Š CSV export functionality  
â”œâ”€â”€ extract_page_data.go       # ğŸ” Page data extraction
â”œâ”€â”€ get_urls.go                # ğŸ”— URL and image extraction
â”œâ”€â”€ get_html.go                # ğŸŒ HTTP client for fetching pages
â”œâ”€â”€ normalize_url.go           # ğŸ§¹ URL normalization utilities
â”œâ”€â”€ *_test.go                  # ğŸ§ª Test files
â”œâ”€â”€ go.mod                     # ğŸ“¦ Go module definition
â””â”€â”€ README.md                  # ğŸ“– This file
```

## ğŸ§  How It Works

1. **ğŸ¯ Target Selection** - Starts with the provided URL and parses the domain
2. **ï¿½ï¿½ Concurrent Crawling** - Spawns goroutines up to the concurrency limit  
3. **ğŸ•·ï¸ Page Processing** - For each page:
   - Fetches HTML content
   - Extracts H1, first paragraph, links, and images
   - Finds new URLs to crawl
4. **ğŸ›‘ Smart Limiting** - Stops when max pages reached or no more pages found
5. **ğŸ“Š Data Export** - Saves all structured data to CSV format

## âš™ï¸ Technical Details

### Concurrency Model
- Uses **buffered channels** to limit concurrent requests
- **Mutex-protected** shared data structures
- **WaitGroups** ensure all goroutines complete before exit

### Data Extraction
- **HTML parsing** with goquery (jQuery-like selectors)
- **URL normalization** for deduplication
- **Relative to absolute** URL conversion
- **Domain boundary** enforcement

### Performance
- **Non-blocking I/O** with goroutines
- **Memory efficient** page data storage
- **Configurable concurrency** for different network conditions

## ğŸ§ª Testing

Run the test suite:
```bash
go test -v
```

Test specific functionality:
```bash
go test -run TestAddPageVisit
go test -run TestWriteCSVReport
```

## ğŸ“Š Example Results

After crawling `https://blog.boot.dev/` with 25 pages:

```csv
page_url,h1,first_paragraph,outgoing_link_urls,image_urls
https://blog.boot.dev/,Boot.dev Blog,Learn backend development with our tutorials,https://boot.dev;https://boot.dev/courses,https://blog.boot.dev/logo.png
https://blog.boot.dev/golang/,Go Tutorials,Master Go programming language,https://go.dev;https://boot.dev/courses/go,https://blog.boot.dev/go-logo.png;https://blog.boot.dev/gopher.jpg
```

## ğŸš€ Performance Tips

### Concurrency Settings
- **Start with 1-2** for initial testing
- **Use 3-5** for most websites  
- **Up to 10** for high-performance scenarios
- **Be respectful** - don't overwhelm target servers

### Page Limits
- **10-25 pages** for small sites
- **50-100 pages** for medium sites
- **500+ pages** for large sites (be careful!)


